{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8671cd2-37e4-4acc-bc20-9a6b4420b8e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T11:24:16.381136Z",
     "iopub.status.busy": "2024-10-02T11:24:16.380701Z",
     "iopub.status.idle": "2024-10-02T11:24:16.457797Z",
     "shell.execute_reply": "2024-10-02T11:24:16.457243Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "# IP列表\n",
    "'''\n",
    "IP_LIST = ['20.213.247.195','13.74.59.33','145.40.121.101','45.167.125.97','157.100.12.138',\n",
    "           '173.244.48.9','51.79.205.165','190.15.103.66','45.42.177.50','8.219.64.236']\n",
    "\n",
    "def get_random_proxy():\n",
    "    ip = random.choice(IP_LIST)\n",
    "    return {\"http\": f\"http://{ip}\", \"https\": f\"http://{ip}\"}\n",
    "'''\n",
    "\n",
    "def init_db():\n",
    "    conn = sqlite3.connect('web_info.db')  # 更改為 web_info.db\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS web_info (\n",
    "            work_ID TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            tags TEXT,\n",
    "            like_count INTEGER,\n",
    "            bookmark_count INTEGER,\n",
    "            view_count INTEGER,\n",
    "            image_url TEXT\n",
    "        )''')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def get_last_work_id(conn):\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT MAX(work_ID) FROM web_info\")\n",
    "    result = c.fetchone()[0]\n",
    "    return result if result else 0\n",
    "\n",
    "connn = init_db()\n",
    "\n",
    "start_id = int(get_last_work_id(connn)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38defef-e43a-4c2f-8607-fe19909046fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T11:24:16.460072Z",
     "iopub.status.busy": "2024-10-02T11:24:16.459684Z",
     "iopub.status.idle": "2024-10-02T11:24:35.876111Z",
     "shell.execute_reply": "2024-10-02T11:24:35.875495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search from ID: 121188809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs:   0%|          | 0/3 [00:00<?, ?ID/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs:  33%|███▎      | 1/3 [00:00<00:00,  2.29ID/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs:  33%|███▎      | 1/3 [00:00<00:00,  2.29ID/s, Latest ID: 121188809]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs:  67%|██████▋   | 2/3 [00:11<00:06,  6.41s/ID, Latest ID: 121188809]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs:  67%|██████▋   | 2/3 [00:11<00:06,  6.41s/ID, Latest ID: 121188810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs: 100%|██████████| 3/3 [00:19<00:00,  7.30s/ID, Latest ID: 121188810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs: 100%|██████████| 3/3 [00:19<00:00,  7.30s/ID, Latest ID: 121188811]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Finding valid work IDs: 100%|██████████| 3/3 [00:19<00:00,  6.47s/ID, Latest ID: 121188811]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully found 50 valid work IDs.\n",
      "Valid work IDs: [121188809, 121188810, 121188811]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_pixiv_work_ids(start_id, max_attempts=1000):\n",
    "    valid_ids = []\n",
    "    \n",
    "    # 創建tqdm進度條\n",
    "    with tqdm(total=3, desc=\"Finding valid work IDs\", unit=\"ID\") as pbar:\n",
    "        for attempt in range(max_attempts):\n",
    "            work_id = start_id + attempt\n",
    "            url = f\"https://www.pixiv.net/artworks/{work_id}\"\n",
    "            \n",
    "            try:\n",
    "                #proxy = get_random_proxy()\n",
    "                response = requests.head(url)\n",
    "                if response.status_code == 200:\n",
    "                    valid_ids.append(work_id)\n",
    "                    pbar.update(1)  # 更新進度條\n",
    "                    pbar.set_postfix_str(f\"Latest ID: {work_id}\")  # 顯示最新找到的ID\n",
    "                \n",
    "                if len(valid_ids) == 3:\n",
    "                    break\n",
    "                \n",
    "                # 添加延遲以避免過於頻繁的請求\n",
    "                delay = random.uniform(5, 15)  # 5到15秒的隨機延遲\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            except requests.RequestException as e:\n",
    "                tqdm.write(f\"Error checking work ID {work_id}: {e}\")\n",
    "    \n",
    "    if len(valid_ids) == 3:\n",
    "        print(\"\\nSuccessfully found 50 valid work IDs.\")\n",
    "    else:\n",
    "        print(f\"\\nReached maximum attempts. Found {len(valid_ids)} valid work IDs.\")\n",
    "    \n",
    "    return valid_ids\n",
    "\n",
    "# 使用函數\n",
    "#start_id = 121188700  # 隨機選擇起始ID\n",
    "print(f\"Starting search from ID: {start_id}\")\n",
    "result = check_pixiv_work_ids(start_id)\n",
    "print(\"Valid work IDs:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a673e5-8c78-46cf-878e-b0312b5eb234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T11:24:35.878140Z",
     "iopub.status.busy": "2024-10-02T11:24:35.877698Z",
     "iopub.status.idle": "2024-10-02T11:24:35.936735Z",
     "shell.execute_reply": "2024-10-02T11:24:35.936211Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def save_webpage_as_single_file(url, filename):\n",
    "    try:\n",
    "        # 发送 GET 请求获取网页内容\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        #time.sleep(np.random.randint(3,7))\n",
    "\n",
    "        # 解析网页内容\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 获取网页的标题\n",
    "        title = soup.title.string if soup.title else 'webpage'\n",
    "        \n",
    "        # 创建 .mhtml 文件内容\n",
    "        mhtml_content = f\"<!DOCTYPE html>\\n<html>\\n<head>\\n<title>{title}</title>\\n</head>\\n<body>\\n\"\n",
    "        mhtml_content += str(soup)  # 添加网页内容\n",
    "        mhtml_content += \"\\n</body>\\n</html>\"\n",
    "\n",
    "        # 保存为 .mhtml 文件\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(mhtml_content)\n",
    "\n",
    "        print(f\"网页内容已成功保存为 {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求失败: {e}\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"保存文件时出错: {e}\")\n",
    "\n",
    "# 输入 URL 和文件名\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0078b3f-3262-4e94-857f-53a9e79bac74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T11:24:35.938816Z",
     "iopub.status.busy": "2024-10-02T11:24:35.938474Z",
     "iopub.status.idle": "2024-10-02T11:24:35.944902Z",
     "shell.execute_reply": "2024-10-02T11:24:35.944452Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "# 提取信息的函数\n",
    "def extract_info_from_mhtml(mhtml_file,work_ID):\n",
    "    with open(mhtml_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 使用 BeautifulSoup 解析 HTML 内容\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # 提取 <title> 标签的内容\n",
    "    title = soup.title.string if soup.title else '无标题'\n",
    "    \n",
    "     # 使用正则表达式提取多个 \"tag\"\n",
    "    tag_pattern = re.findall(r'\"tag\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "    \n",
    "    # 使用正则表达式提取 \"likeCount\", \"bookmarkCount\", \"viewCount\"\n",
    "    like_count_pattern = re.search(r'\"likeCount\"\\s*:\\s*(\\d+)', content)\n",
    "    bookmark_count_pattern = re.search(r'\"bookmarkCount\"\\s*:\\s*(\\d+)', content)\n",
    "    view_count_pattern = re.search(r'\"viewCount\"\\s*:\\s*(\\d+)', content)\n",
    "    image_pattern = re.search(r'\"regular\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "\n",
    "    # 获取正则表达式的匹配结果\n",
    "    tags = tag_pattern if tag_pattern else ['无标签']\n",
    "    like_count = like_count_pattern.group(1) if like_count_pattern else '无点赞数'\n",
    "    bookmark_count = bookmark_count_pattern.group(1) if bookmark_count_pattern else '无收藏数'\n",
    "    view_count = view_count_pattern.group(1) if view_count_pattern else '无浏览数'\n",
    "    image_count = image_pattern.group(1) if image_pattern else '無影像連結'\n",
    "    return {\n",
    "        'work_ID': str(work_ID),\n",
    "        'title': title.split(' - ')[0],  # 清理标题内容\n",
    "        'tags': ', '.join(tags),  # 标签列表转为字符串\n",
    "        'like_count': like_count,\n",
    "        'bookmark_count': bookmark_count,\n",
    "        'view_count': view_count,\n",
    "        'image url': image_count\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# 将数据追加到 SQLite 数据库的函数\n",
    "def append_to_sqlite(info, db_filename='web_info.db'):\n",
    "    # 连接 SQLite 数据库（如果文件不存在，会自动创建）\n",
    "    conn = sqlite3.connect(db_filename)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 创建表格（如果不存在）\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS web_info (\n",
    "            work_ID TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            tags TEXT,\n",
    "            like_count INTEGER,\n",
    "            bookmark_count INTEGER,\n",
    "            view_count INTEGER,\n",
    "            image_url TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # 插入数据，如果 work_ID 已存在则忽略插入\n",
    "    cursor.execute('''\n",
    "        INSERT OR REPLACE INTO web_info (work_ID, title, tags, like_count, bookmark_count, view_count, image_url)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        info['work_ID'],\n",
    "        info['title'],\n",
    "        info['tags'],\n",
    "        int(info['like_count']),\n",
    "        int(info['bookmark_count']),\n",
    "        int(info['view_count']),\n",
    "        info['image url']\n",
    "    ))\n",
    "\n",
    "    # 提交更改并关闭连接\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16847b22-1171-48b2-96de-df37fcdbced2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T11:24:35.946840Z",
     "iopub.status.busy": "2024-10-02T11:24:35.946341Z",
     "iopub.status.idle": "2024-10-02T11:27:07.847639Z",
     "shell.execute_reply": "2024-10-02T11:27:07.847004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网页内容已成功保存为 121188809.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 45 秒鐘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网页内容已成功保存为 121188810.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 50 秒鐘\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网页内容已成功保存为 121188811.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 56 秒鐘\n"
     ]
    }
   ],
   "source": [
    "db_filename = 'web_info.db'\n",
    "for work_ID in result:\n",
    "    url = f'https://www.pixiv.net/artworks/{work_ID}'\n",
    "    filename = f'{work_ID}.mhtml'\n",
    "    try:\n",
    "        save_webpage_as_single_file(url, filename)\n",
    "        # 示例：提取和追加数据\n",
    "        mhtml_file = f'{work_ID}.mhtml'  # 替换为实际的 .mhtml 文件路径\n",
    "        info = extract_info_from_mhtml(mhtml_file,work_ID)\n",
    "        # 将提取的信息追加到 'web_info.xlsx'\n",
    "        append_to_sqlite(info)\n",
    "        print(f\"新信息已成功追加到 {db_filename}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    delay = int(random.uniform(30, 60))  # 5到15秒的隨機延遲\n",
    "    print(f\"休息 {delay} 秒鐘\")\n",
    "    time.sleep(delay)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd90b55-71ee-4e77-bd49-7115e3a96b3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T11:27:07.850153Z",
     "iopub.status.busy": "2024-10-02T11:27:07.849912Z",
     "iopub.status.idle": "2024-10-02T11:27:07.854278Z",
     "shell.execute_reply": "2024-10-02T11:27:07.853780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of work_IDs: 87\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# 連接到 SQLite 資料庫\n",
    "con = sqlite3.connect('web_info.db')\n",
    "cur = con.cursor()\n",
    "\n",
    "# 使用 SQL 查詢計算 work_ID 的總數\n",
    "cur.execute(\"SELECT COUNT(work_ID) FROM web_info\")\n",
    "work_id_count = cur.fetchone()[0]\n",
    "\n",
    "# 顯示 work_ID 總數\n",
    "print(f\"Total number of work_IDs: {work_id_count}\")\n",
    "\n",
    "# 關閉資料庫連線\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37104cd1-a3e8-47d3-a466-2b4b61a2c080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-02T11:27:07.856084Z",
     "iopub.status.busy": "2024-10-02T11:27:07.855864Z",
     "iopub.status.idle": "2024-10-02T11:27:07.861930Z",
     "shell.execute_reply": "2024-10-02T11:27:07.861467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\n\\n# 图片的 URL\\nurl = \\'https://i.pximg.net/img-master/img/2024/08/05/22/47/33/121215791_p0_master1200.jpg\\'\\n\\n# 图片保存的本地文件名\\nfilename = \\'downloaded_image.png\\'\\n\\n# 请求头，模拟完整的浏览器请求\\nheaders = {\\n    \\'User-Agent\\': \\'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\\',\\n    \\'Referer\\': \\'https://www.pixiv.net/\\',\\n    \\'Accept\\': \\'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\\',\\n    \\'Accept-Encoding\\': \\'gzip, deflate, br\\',\\n    \\'Accept-Language\\': \\'en-US,en;q=0.9\\',\\n    \\'Connection\\': \\'keep-alive\\'\\n}\\n\\n# 发起 GET 请求以下载图片\\nresponse = requests.get(url, headers=headers)\\n\\n# 检查请求是否成功\\nif response.status_code == 200:\\n    # 将图片数据写入本地文件\\n    with open(filename, \\'wb\\') as file:\\n        file.write(response.content)\\n    print(f\"图片已成功保存为 {filename}\")\\nelse:\\n    print(f\"图片下载失败，状态码：{response.status_code}\")\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import requests\n",
    "\n",
    "# 图片的 URL\n",
    "url = 'https://i.pximg.net/img-master/img/2024/08/05/22/47/33/121215791_p0_master1200.jpg'\n",
    "\n",
    "# 图片保存的本地文件名\n",
    "filename = 'downloaded_image.png'\n",
    "\n",
    "# 请求头，模拟完整的浏览器请求\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36',\n",
    "    'Referer': 'https://www.pixiv.net/',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "# 发起 GET 请求以下载图片\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 检查请求是否成功\n",
    "if response.status_code == 200:\n",
    "    # 将图片数据写入本地文件\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"图片已成功保存为 {filename}\")\n",
    "else:\n",
    "    print(f\"图片下载失败，状态码：{response.status_code}\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662c282-045f-46fe-9c92-afe79094a532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
