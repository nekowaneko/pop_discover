{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8671cd2-37e4-4acc-bc20-9a6b4420b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#work_ID = 121188719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b38defef-e43a-4c2f-8607-fe19909046fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search from ID: 121188700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding valid work IDs: 100%|████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/ID, Latest ID: 121188710]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully found 10 valid work IDs.\n",
      "Valid work IDs: [121188700, 121188701, 121188702, 121188703, 121188704, 121188706, 121188707, 121188708, 121188709, 121188710]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def check_pixiv_work_ids(start_id, max_attempts=1000):\n",
    "    valid_ids = []\n",
    "    \n",
    "    # 創建tqdm進度條\n",
    "    with tqdm(total=10, desc=\"Finding valid work IDs\", unit=\"ID\") as pbar:\n",
    "        for attempt in range(max_attempts):\n",
    "            work_id = start_id + attempt\n",
    "            url = f\"https://www.pixiv.net/artworks/{work_id}\"\n",
    "            \n",
    "            try:\n",
    "                response = requests.head(url)\n",
    "                if response.status_code == 200:\n",
    "                    valid_ids.append(work_id)\n",
    "                    pbar.update(1)  # 更新進度條\n",
    "                    pbar.set_postfix_str(f\"Latest ID: {work_id}\")  # 顯示最新找到的ID\n",
    "                \n",
    "                if len(valid_ids) == 10:\n",
    "                    break\n",
    "                \n",
    "                # 添加延遲以避免過於頻繁的請求\n",
    "                time.sleep(1)\n",
    "            \n",
    "            except requests.RequestException as e:\n",
    "                tqdm.write(f\"Error checking work ID {work_id}: {e}\")\n",
    "    \n",
    "    if len(valid_ids) == 10:\n",
    "        print(\"\\nSuccessfully found 10 valid work IDs.\")\n",
    "    else:\n",
    "        print(f\"\\nReached maximum attempts. Found {len(valid_ids)} valid work IDs.\")\n",
    "    \n",
    "    return valid_ids\n",
    "\n",
    "# 使用函數\n",
    "start_id = 121188700  # 隨機選擇起始ID\n",
    "print(f\"Starting search from ID: {start_id}\")\n",
    "result = check_pixiv_work_ids(start_id)\n",
    "print(\"Valid work IDs:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93a673e5-8c78-46cf-878e-b0312b5eb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def save_webpage_as_single_file(url, filename):\n",
    "    try:\n",
    "        # 发送 GET 请求获取网页内容\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "\n",
    "        # 解析网页内容\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 获取网页的标题\n",
    "        title = soup.title.string if soup.title else 'webpage'\n",
    "        \n",
    "        # 创建 .mhtml 文件内容\n",
    "        mhtml_content = f\"<!DOCTYPE html>\\n<html>\\n<head>\\n<title>{title}</title>\\n</head>\\n<body>\\n\"\n",
    "        mhtml_content += str(soup)  # 添加网页内容\n",
    "        mhtml_content += \"\\n</body>\\n</html>\"\n",
    "\n",
    "        # 保存为 .mhtml 文件\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(mhtml_content)\n",
    "\n",
    "        print(f\"网页内容已成功保存为 {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求失败: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存文件时出错: {e}\")\n",
    "\n",
    "# 输入 URL 和文件名\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0078b3f-3262-4e94-857f-53a9e79bac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 提取信息的函数\n",
    "def extract_info_from_mhtml(mhtml_file,work_ID):\n",
    "    with open(mhtml_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 使用 BeautifulSoup 解析 HTML 内容\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # 提取 <title> 标签的内容\n",
    "    title = soup.title.string if soup.title else '无标题'\n",
    "    \n",
    "     # 使用正则表达式提取多个 \"tag\"\n",
    "    tag_pattern = re.findall(r'\"tag\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "    \n",
    "    # 使用正则表达式提取 \"likeCount\", \"bookmarkCount\", \"viewCount\"\n",
    "    like_count_pattern = re.search(r'\"likeCount\"\\s*:\\s*(\\d+)', content)\n",
    "    bookmark_count_pattern = re.search(r'\"bookmarkCount\"\\s*:\\s*(\\d+)', content)\n",
    "    view_count_pattern = re.search(r'\"viewCount\"\\s*:\\s*(\\d+)', content)\n",
    "    image_pattern = re.search(r'\"regular\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "\n",
    "    # 获取正则表达式的匹配结果\n",
    "    tags = tag_pattern if tag_pattern else ['无标签']\n",
    "    like_count = like_count_pattern.group(1) if like_count_pattern else '无点赞数'\n",
    "    bookmark_count = bookmark_count_pattern.group(1) if bookmark_count_pattern else '无收藏数'\n",
    "    view_count = view_count_pattern.group(1) if view_count_pattern else '无浏览数'\n",
    "    image_count = image_pattern.group(1) if image_pattern else '無影像連結'\n",
    "    return {\n",
    "        'work_ID': str(work_ID),\n",
    "        'title': title.split(' - ')[0],  # 清理标题内容\n",
    "        'tags': ', '.join(tags),  # 标签列表转为字符串\n",
    "        'like_count': like_count,\n",
    "        'bookmark_count': bookmark_count,\n",
    "        'view_count': view_count,\n",
    "        'image url': image_count\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# 将数据追加到 Excel 文件的函数\n",
    "def append_to_excel(info, excel_filename='web_info.xlsx'):\n",
    "    # 如果 Excel 文件不存在，创建并写入数据\n",
    "    if not os.path.exists(excel_filename):\n",
    "        df = pd.DataFrame({\n",
    "            'work_ID':[info['work_ID']],\n",
    "            'title': [info['title']],\n",
    "            'tags': [info['tags']],\n",
    "            'like_count': [int(info['like_count'])],\n",
    "            'bookmark_count': [int(info['bookmark_count'])],\n",
    "            'view_count': [int(info['view_count'])],\n",
    "            'image url': [info['image url']]\n",
    "        })\n",
    "        df.to_excel(excel_filename, index=False)\n",
    "    else:\n",
    "        # 如果 Excel 文件已经存在，追加新数据\n",
    "        existing_df = pd.read_excel(excel_filename)\n",
    "\n",
    "        # 创建新的 DataFrame\n",
    "        new_df = pd.DataFrame({\n",
    "            'work_ID':[info['work_ID']],\n",
    "            'title': [info['title']],\n",
    "            'tags': [info['tags']],\n",
    "            'like_count': [int(info['like_count'])],\n",
    "            'bookmark_count': [int(info['bookmark_count'])],\n",
    "            'view_count': [int(info['view_count'])],\n",
    "            'image url': [info['image url']]\n",
    "        })\n",
    "\n",
    "        # 追加新数据到现有的 DataFrame 中\n",
    "        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "        # 写回到 Excel 文件\n",
    "        updated_df.to_excel(excel_filename, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16847b22-1171-48b2-96de-df37fcdbced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网页内容已成功保存为 121188700.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188701.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188702.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188703.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188704.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188706.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188707.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188708.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188709.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n",
      "网页内容已成功保存为 121188710.mhtml\n",
      "新信息已成功追加到 web_info.xlsx\n"
     ]
    }
   ],
   "source": [
    "for work_ID in result:\n",
    "    url = f'https://www.pixiv.net/artworks/{work_ID}'\n",
    "    filename = f'{work_ID}.mhtml'\n",
    "    save_webpage_as_single_file(url, filename)\n",
    "    # 示例：提取和追加数据\n",
    "    mhtml_file = f'{work_ID}.mhtml'  # 替换为实际的 .mhtml 文件路径\n",
    "    info = extract_info_from_mhtml(mhtml_file,work_ID)\n",
    "\n",
    "    # 将提取的信息追加到 'web_info.xlsx'\n",
    "    append_to_excel(info)\n",
    "\n",
    "    print(f\"新信息已成功追加到 {excel_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37104cd1-a3e8-47d3-a466-2b4b61a2c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已成功保存为 downloaded_image.png\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import requests\n",
    "\n",
    "# 图片的 URL\n",
    "url = 'https://i.pximg.net/img-master/img/2024/08/05/22/47/33/121215791_p0_master1200.jpg'\n",
    "\n",
    "# 图片保存的本地文件名\n",
    "filename = 'downloaded_image.png'\n",
    "\n",
    "# 请求头，模拟完整的浏览器请求\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36',\n",
    "    'Referer': 'https://www.pixiv.net/',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "# 发起 GET 请求以下载图片\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 检查请求是否成功\n",
    "if response.status_code == 200:\n",
    "    # 将图片数据写入本地文件\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"图片已成功保存为 {filename}\")\n",
    "else:\n",
    "    print(f\"图片下载失败，状态码：{response.status_code}\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662c282-045f-46fe-9c92-afe79094a532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
