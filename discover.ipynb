{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8671cd2-37e4-4acc-bc20-9a6b4420b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "# IP列表\n",
    "'''\n",
    "IP_LIST = ['20.213.247.195','13.74.59.33','145.40.121.101','45.167.125.97','157.100.12.138',\n",
    "           '173.244.48.9','51.79.205.165','190.15.103.66','45.42.177.50','8.219.64.236']\n",
    "\n",
    "def get_random_proxy():\n",
    "    ip = random.choice(IP_LIST)\n",
    "    return {\"http\": f\"http://{ip}\", \"https\": f\"http://{ip}\"}\n",
    "'''\n",
    "\n",
    "def init_db():\n",
    "    conn = sqlite3.connect('web_info.db')  # 更改為 web_info.db\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS web_info (\n",
    "            work_ID TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            tags TEXT,\n",
    "            like_count INTEGER,\n",
    "            bookmark_count INTEGER,\n",
    "            view_count INTEGER,\n",
    "            image_url TEXT\n",
    "        )''')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def get_last_work_id(conn):\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT MAX(work_ID) FROM web_info\")\n",
    "    result = c.fetchone()[0]\n",
    "    return result if result else 0\n",
    "\n",
    "connn = init_db()\n",
    "\n",
    "start_id = int(get_last_work_id(connn)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38defef-e43a-4c2f-8607-fe19909046fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting search from ID: 121188759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding valid work IDs: 100%|████████████████████████████████████| 50/50 [11:57<00:00, 14.34s/ID, Latest ID: 121188828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully found 10 valid work IDs.\n",
      "Valid work IDs: [121188760, 121188761, 121188762, 121188764, 121188766, 121188767, 121188768, 121188773, 121188774, 121188775, 121188776, 121188777, 121188778, 121188779, 121188780, 121188781, 121188782, 121188783, 121188784, 121188785, 121188787, 121188789, 121188791, 121188794, 121188796, 121188797, 121188798, 121188799, 121188800, 121188802, 121188804, 121188805, 121188807, 121188808, 121188809, 121188810, 121188811, 121188813, 121188815, 121188816, 121188818, 121188819, 121188820, 121188821, 121188822, 121188823, 121188824, 121188825, 121188826, 121188828]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def check_pixiv_work_ids(start_id, max_attempts=1000):\n",
    "    valid_ids = []\n",
    "    \n",
    "    # 創建tqdm進度條\n",
    "    with tqdm(total=3, desc=\"Finding valid work IDs\", unit=\"ID\") as pbar:\n",
    "        for attempt in range(max_attempts):\n",
    "            work_id = start_id + attempt\n",
    "            url = f\"https://www.pixiv.net/artworks/{work_id}\"\n",
    "            \n",
    "            try:\n",
    "                #proxy = get_random_proxy()\n",
    "                response = requests.head(url)\n",
    "                if response.status_code == 200:\n",
    "                    valid_ids.append(work_id)\n",
    "                    pbar.update(1)  # 更新進度條\n",
    "                    pbar.set_postfix_str(f\"Latest ID: {work_id}\")  # 顯示最新找到的ID\n",
    "                \n",
    "                if len(valid_ids) == 3:\n",
    "                    break\n",
    "                \n",
    "                # 添加延遲以避免過於頻繁的請求\n",
    "                delay = random.uniform(5, 15)  # 5到15秒的隨機延遲\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            except requests.RequestException as e:\n",
    "                tqdm.write(f\"Error checking work ID {work_id}: {e}\")\n",
    "    \n",
    "    if len(valid_ids) == 3:\n",
    "        print(\"\\nSuccessfully found 50 valid work IDs.\")\n",
    "    else:\n",
    "        print(f\"\\nReached maximum attempts. Found {len(valid_ids)} valid work IDs.\")\n",
    "    \n",
    "    return valid_ids\n",
    "\n",
    "# 使用函數\n",
    "#start_id = 121188700  # 隨機選擇起始ID\n",
    "print(f\"Starting search from ID: {start_id}\")\n",
    "result = check_pixiv_work_ids(start_id)\n",
    "print(\"Valid work IDs:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a673e5-8c78-46cf-878e-b0312b5eb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def save_webpage_as_single_file(url, filename):\n",
    "    try:\n",
    "        # 发送 GET 请求获取网页内容\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        time.sleep(np.random.randint(3,7))\n",
    "\n",
    "        # 解析网页内容\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 获取网页的标题\n",
    "        title = soup.title.string if soup.title else 'webpage'\n",
    "        \n",
    "        # 创建 .mhtml 文件内容\n",
    "        mhtml_content = f\"<!DOCTYPE html>\\n<html>\\n<head>\\n<title>{title}</title>\\n</head>\\n<body>\\n\"\n",
    "        mhtml_content += str(soup)  # 添加网页内容\n",
    "        mhtml_content += \"\\n</body>\\n</html>\"\n",
    "\n",
    "        # 保存为 .mhtml 文件\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(mhtml_content)\n",
    "\n",
    "        print(f\"网页内容已成功保存为 {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求失败: {e}\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"保存文件时出错: {e}\")\n",
    "\n",
    "# 输入 URL 和文件名\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0078b3f-3262-4e94-857f-53a9e79bac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "# 提取信息的函数\n",
    "def extract_info_from_mhtml(mhtml_file,work_ID):\n",
    "    with open(mhtml_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 使用 BeautifulSoup 解析 HTML 内容\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # 提取 <title> 标签的内容\n",
    "    title = soup.title.string if soup.title else '无标题'\n",
    "    \n",
    "     # 使用正则表达式提取多个 \"tag\"\n",
    "    tag_pattern = re.findall(r'\"tag\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "    \n",
    "    # 使用正则表达式提取 \"likeCount\", \"bookmarkCount\", \"viewCount\"\n",
    "    like_count_pattern = re.search(r'\"likeCount\"\\s*:\\s*(\\d+)', content)\n",
    "    bookmark_count_pattern = re.search(r'\"bookmarkCount\"\\s*:\\s*(\\d+)', content)\n",
    "    view_count_pattern = re.search(r'\"viewCount\"\\s*:\\s*(\\d+)', content)\n",
    "    image_pattern = re.search(r'\"regular\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "\n",
    "    # 获取正则表达式的匹配结果\n",
    "    tags = tag_pattern if tag_pattern else ['无标签']\n",
    "    like_count = like_count_pattern.group(1) if like_count_pattern else '无点赞数'\n",
    "    bookmark_count = bookmark_count_pattern.group(1) if bookmark_count_pattern else '无收藏数'\n",
    "    view_count = view_count_pattern.group(1) if view_count_pattern else '无浏览数'\n",
    "    image_count = image_pattern.group(1) if image_pattern else '無影像連結'\n",
    "    return {\n",
    "        'work_ID': str(work_ID),\n",
    "        'title': title.split(' - ')[0],  # 清理标题内容\n",
    "        'tags': ', '.join(tags),  # 标签列表转为字符串\n",
    "        'like_count': like_count,\n",
    "        'bookmark_count': bookmark_count,\n",
    "        'view_count': view_count,\n",
    "        'image url': image_count\n",
    "        \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# 将数据追加到 SQLite 数据库的函数\n",
    "def append_to_sqlite(info, db_filename='web_info.db'):\n",
    "    # 连接 SQLite 数据库（如果文件不存在，会自动创建）\n",
    "    conn = sqlite3.connect(db_filename)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 创建表格（如果不存在）\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS web_info (\n",
    "            work_ID TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            tags TEXT,\n",
    "            like_count INTEGER,\n",
    "            bookmark_count INTEGER,\n",
    "            view_count INTEGER,\n",
    "            image_url TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # 插入数据，如果 work_ID 已存在则忽略插入\n",
    "    cursor.execute('''\n",
    "        INSERT OR REPLACE INTO web_info (work_ID, title, tags, like_count, bookmark_count, view_count, image_url)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        info['work_ID'],\n",
    "        info['title'],\n",
    "        info['tags'],\n",
    "        int(info['like_count']),\n",
    "        int(info['bookmark_count']),\n",
    "        int(info['view_count']),\n",
    "        info['image url']\n",
    "    ))\n",
    "\n",
    "    # 提交更改并关闭连接\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16847b22-1171-48b2-96de-df37fcdbced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网页内容已成功保存为 121188760.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 6.754498094595637 秒鐘\n",
      "网页内容已成功保存为 121188761.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.917403067211407 秒鐘\n",
      "网页内容已成功保存为 121188762.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 6.153259301508416 秒鐘\n",
      "网页内容已成功保存为 121188764.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 13.358502665267775 秒鐘\n",
      "网页内容已成功保存为 121188766.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 11.41318807805997 秒鐘\n",
      "网页内容已成功保存为 121188767.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 8.826034061001003 秒鐘\n",
      "网页内容已成功保存为 121188768.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 11.817655975047291 秒鐘\n",
      "网页内容已成功保存为 121188773.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.894613467934938 秒鐘\n",
      "网页内容已成功保存为 121188774.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.508366514623481 秒鐘\n",
      "网页内容已成功保存为 121188775.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.744704450004734 秒鐘\n",
      "网页内容已成功保存为 121188776.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 5.995967471400619 秒鐘\n",
      "网页内容已成功保存为 121188777.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 8.499119153516995 秒鐘\n",
      "网页内容已成功保存为 121188778.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 14.196579200872668 秒鐘\n",
      "网页内容已成功保存为 121188779.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 11.467481540643316 秒鐘\n",
      "网页内容已成功保存为 121188780.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 6.256739920347888 秒鐘\n",
      "网页内容已成功保存为 121188781.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 10.088258796601059 秒鐘\n",
      "网页内容已成功保存为 121188782.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 11.00734099332428 秒鐘\n",
      "网页内容已成功保存为 121188783.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.854567092963645 秒鐘\n",
      "网页内容已成功保存为 121188784.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 10.769044380352838 秒鐘\n",
      "网页内容已成功保存为 121188785.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.744820567035639 秒鐘\n",
      "网页内容已成功保存为 121188787.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 9.20335051644377 秒鐘\n",
      "网页内容已成功保存为 121188789.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 9.672523277618119 秒鐘\n",
      "网页内容已成功保存为 121188791.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 14.003933078628377 秒鐘\n",
      "网页内容已成功保存为 121188794.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 5.420921012682941 秒鐘\n",
      "网页内容已成功保存为 121188796.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 6.0717043948907 秒鐘\n",
      "网页内容已成功保存为 121188797.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.181439919885428 秒鐘\n",
      "网页内容已成功保存为 121188798.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 8.09316103813667 秒鐘\n",
      "网页内容已成功保存为 121188799.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 13.963860391571576 秒鐘\n",
      "网页内容已成功保存为 121188800.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 6.7279525140602345 秒鐘\n",
      "网页内容已成功保存为 121188802.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 14.832129919964197 秒鐘\n",
      "网页内容已成功保存为 121188804.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 6.274176160026231 秒鐘\n",
      "网页内容已成功保存为 121188805.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 11.639282305840123 秒鐘\n",
      "网页内容已成功保存为 121188807.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 9.204791006892886 秒鐘\n",
      "网页内容已成功保存为 121188808.mhtml\n",
      "新信息已成功追加到 web_info.db\n",
      "休息 7.976176279266768 秒鐘\n",
      "请求失败: ('Connection aborted.', ConnectionResetError(10054, '遠端主機已強制關閉一個現存的連線。', None, 10054, None))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '121188809.mhtml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2040\\1444849715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# 示例：提取和追加数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmhtml_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{work_ID}.mhtml'\u001b[0m  \u001b[1;31m# 替换为实际的 .mhtml 文件路径\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_info_from_mhtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmhtml_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwork_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 将提取的信息追加到 'web_info.xlsx'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mappend_to_sqlite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2040\\21151283.py\u001b[0m in \u001b[0;36mextract_info_from_mhtml\u001b[1;34m(mhtml_file, work_ID)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 提取信息的函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_info_from_mhtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmhtml_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwork_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmhtml_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '121188809.mhtml'"
     ]
    }
   ],
   "source": [
    "db_filename = 'web_info.db'\n",
    "for work_ID in result:\n",
    "    url = f'https://www.pixiv.net/artworks/{work_ID}'\n",
    "    filename = f'{work_ID}.mhtml'\n",
    "    try:\n",
    "        save_webpage_as_single_file(url, filename)\n",
    "        # 示例：提取和追加数据\n",
    "        mhtml_file = f'{work_ID}.mhtml'  # 替换为实际的 .mhtml 文件路径\n",
    "        info = extract_info_from_mhtml(mhtml_file,work_ID)\n",
    "        # 将提取的信息追加到 'web_info.xlsx'\n",
    "        append_to_sqlite(info)\n",
    "        print(f\"新信息已成功追加到 {db_filename}\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    delay = random.uniform(30, 60)  # 5到15秒的隨機延遲\n",
    "    print(f\"休息 {delay} 秒鐘\")\n",
    "    time.sleep(delay)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd90b55-71ee-4e77-bd49-7115e3a96b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of work_IDs: 84\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# 連接到 SQLite 資料庫\n",
    "con = sqlite3.connect('web_info.db')\n",
    "cur = con.cursor()\n",
    "\n",
    "# 使用 SQL 查詢計算 work_ID 的總數\n",
    "cur.execute(\"SELECT COUNT(work_ID) FROM web_info\")\n",
    "work_id_count = cur.fetchone()[0]\n",
    "\n",
    "# 顯示 work_ID 總數\n",
    "print(f\"Total number of work_IDs: {work_id_count}\")\n",
    "\n",
    "# 關閉資料庫連線\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37104cd1-a3e8-47d3-a466-2b4b61a2c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片已成功保存为 downloaded_image.png\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import requests\n",
    "\n",
    "# 图片的 URL\n",
    "url = 'https://i.pximg.net/img-master/img/2024/08/05/22/47/33/121215791_p0_master1200.jpg'\n",
    "\n",
    "# 图片保存的本地文件名\n",
    "filename = 'downloaded_image.png'\n",
    "\n",
    "# 请求头，模拟完整的浏览器请求\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36',\n",
    "    'Referer': 'https://www.pixiv.net/',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "# 发起 GET 请求以下载图片\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# 检查请求是否成功\n",
    "if response.status_code == 200:\n",
    "    # 将图片数据写入本地文件\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"图片已成功保存为 {filename}\")\n",
    "else:\n",
    "    print(f\"图片下载失败，状态码：{response.status_code}\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662c282-045f-46fe-9c92-afe79094a532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
